# Raja Sekhar Reddy Gajjala


**Lead Data Engineer | Architect of Scalable Data Solutions | AWS | PySpark | Snowflake | Databricks | AI/ML Innovation | $50K/Mo Cost Savings**

[![Email](https://img.shields.io/badge/Email-shekhar.rj@outlook.com-0078D4?logo=microsoft-outlook)](mailto:shekhar.rj@outlook.com)
[![LinkedIn](https://img.shields.io/badge/-Connect%20on%20LinkedIn-0A66C2?logo=linkedin)](https://www.linkedin.com/in/sekhar546/)
[![GitHub](https://img.shields.io/badge/-Explore%20My%20Code-181717?logo=github)](https://github.com/sekhar546)
[![Calendar](https://img.shields.io/badge/Schedule%20Chat-Book%20Meeting-008272?logo=google-chat)](https://calendly.com/sekhar546/30min)
[![Visitors](https://komarev.com/ghpvc/?username=sekhar546&label=Profile%20Views&color=0e75b6&style=flat)](https://github.com/sekhar546)

---

## About Me

- Technology Lead – Data Engineering with 13+ years of international experience delivering high-impact data solutions for complex business challenges in healthcare and insurance domains.
- Proven expertise in architecting and implementing scalable data pipelines, optimizing cloud infrastructure, and data-driven decision-making using AWS, Azure, PySpark, Snowflake, Databricks, Hadoop, and Git.
- Successfully deployed onshore in the United States for over two years with Optum Global Solutions, collaborating directly with US clients to deliver mission-critical healthcare data projects and ensuring alignment with North American business standards.
- Recognized for delivering robust, cost-effective, and high-performance data ecosystems, achieving operational cost reductions of $50K/month and improving system reliability and business growth.
- Expanded expertise in modern data platforms by independently learning Databricks (unified analytics and data engineering), Apache Iceberg tables and Snowflake (cloud-native data warehousing), gaining hands-on experience with their core features, architectures, and best practices for analytics and ETL.
- Demonstrated leadership in managing cross-functional teams, mentoring junior engineers, and fostering a culture of continuous learning, innovation, and Agile best practices.
- Acquired proficiency in DuckDB and Astral UV for Python, enhancing capabilities in lightweight analytics and high-performance asynchronous programming.
- Gained exposure with AI technologies, including deploying and running large language models (LLMs) such as DeepSeek, Mistral, Gemma, and Phi-4 locally using Ollama, and interacting with them via Open WebUI.
- Automated tasks and workflows by integrating n8n with AI LLMs, streamlining processes and enabling scalable automation.
- Undergoing advanced Python skills for AI integration, including using Ollama, HuggingFace Transformers, and LangChain to interact programmatically with LLMs and build AI-powered solutions.
- Experienced in end-to-end ETL development, cloud cost optimization, and advanced reporting using Talend, Crystal Reports, Tableau, Power BI, and SQL.
- Strategic thinker with strong leadership, Agile, and DevOps skills, committed to transforming raw data into actionable business insights for multi-national organizations.

---

## Technologies & Tools

### Cloud Platforms
![AWS](https://img.shields.io/badge/AWS-232F3E?logo=amazon-aws&logoColor=white)
![Azure](https://img.shields.io/badge/Azure-0078D4?logo=microsoft-azure&logoColor=white)
![Databricks](https://img.shields.io/badge/Databricks-E91E63?logo=databricks&logoColor=white)
![Snowflake](https://img.shields.io/badge/Snowflake-29B5E8?logo=snowflake&logoColor=white)

### Languages & Frameworks
![Apache Spark](https://img.shields.io/badge/Apache_Spark-E25A1C?logo=apache-spark&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white)
![SQL](https://img.shields.io/badge/SQL-4479A1?logo=postgresql&logoColor=white)

### Data Engineering
![Hadoop](https://img.shields.io/badge/Hadoop-66CCFF?logo=apache-hadoop&logoColor=black)
![Apache Airflow](https://img.shields.io/badge/Apache_Airflow-017CEE?logo=apache-airflow&logoColor=white)
![DuckDB](https://img.shields.io/badge/DuckDB-DD3333?logo=duckdb&logoColor=white)
![Astral UV](https://img.shields.io/badge/Astral_UV-663399?logo=python&logoColor=white)
![Apache Iceberg](https://img.shields.io/badge/Apache_Iceberg-0078D4?logo=apache&logoColor=white)
![dbt](https://img.shields.io/badge/dbt-FF694B?logo=dbt&logoColor=white)

### Visualization & ETL
![Tableau](https://img.shields.io/badge/Tableau-E97627?logo=tableau&logoColor=white)
![Talend](https://img.shields.io/badge/Talend-FF6D70?logo=talend&logoColor=white)

### DevOps
![Docker](https://img.shields.io/badge/Docker-2496ED?logo=docker&logoColor=white)
![Terraform](https://img.shields.io/badge/Terraform-7B42BC?logo=terraform&logoColor=white)
![Git](https://img.shields.io/badge/Git-F05032?logo=git&logoColor=white)

---

## Professional Experience

**ValueMomentum (November 2020 - June 2024)**
- Architected and implemented a fully automated data pipeline integrating upstream and downstream systems (SnowFlake) using AWS Lambda, PySpark, and EMR, eliminating manual interventions and improving operational efficiency by 40%.
- Proposed the implementation of Databricks within the project and conducted in-depth research on integrating Apache Iceberg tables, evaluating their compatibility with the existing architecture. Documented the advantages—such as support for open table formats, ACID transactions, schema and partition evolution, and vendor neutrality—and potential challenges to inform decision-making on adopting these technologies.
- Designed and implemented ETL workflows to extract, transform, and load data from S3 into AWS Redshift, enabling high-performance analytics and reporting for business stakeholders.
- Built reusable PySpark modules and Python frameworks for data validation, quality assurance, and transformation, supporting evolving business requirements and ensuring data integrity.
- Leveraged AWS Athena to enable ad hoc querying and rapid data exploration directly on S3-stored datasets, reducing time-to-insight for data analysts and business users.
- Automated data ingestion and transformation processes using AWS Glue, integrating data from multiple sources into Redshift and S3, and orchestrated workflows with Airflow for reliability and transparency.
- Pioneered cost optimization strategies by leveraging spot instances, graviton processors, and managed scaling, optimized Redshift clusters for performance and cost, including workload management, partitioning, and data distribution strategies, resulting in a 30% reduction in query latency resulting in monthly savings of $50K while maintaining high system performance.
- Revolutionized Spark processing efficiency by optimizing driver memory configurations in EMR clusters, reducing job failure rates to an industry-leading 0.05% and ensuring seamless data processing.
- Developed automated data ingestion and transformation pipelines, leveraging PySpark for distributed processing and AWS Glue for orchestration, with outputs stored in S3 and loaded into Redshift for high-performance analytics.
- Re-engineered the data pipeline architecture within the Azure cloud ecosystem with Python - PySpark, achieving significant improvements in performance through advanced time and space complexity optimizations.
- Developed data access and governance policies leveraging AWS IAM, S3 bucket policies, and Redshift permissions to ensure compliance and data security.
- Transformed commission payout dashboards in the Azure environment for insurance agents, enabling real-time review of omissions and target planning, which directly contributed to a 15% increase in customer business revenue.
- Spearheaded the onboarding and upskilling of new team members, providing hands-on training in AWS, PySpark, SnowFlake, and Azure, enabling them to contribute to high-impact projects within weeks.
- Mentored junior engineers in best practices for data pipeline development, cloud cost optimization, and Agile methodologies, fostering a culture of continuous learning and innovation.
- Developed and delivered customized training programs on Spark memory optimization, AWS Glue, Databricks, and Snowflake, equipping the team with advanced skills to tackle complex data engineering challenges.
- Championed knowledge-sharing initiatives by conducting workshops on data architecture design and performance tuning, resulting in a 30% improvement in team productivity.
- Built a collaborative learning environment by guiding team members through real-world use cases, enabling them to master Lambda functions, EMR clusters, and SnowFlake integrations.
- Led code version control initiatives by managing the team’s codebase in GitLab, implementing a streamlined branching strategy to reduce merge conflicts and save significant time for the DevOps team.
- Introduced and mentored the team on using graphical tools for version control (e.g., VSCode with Git extensions), establishing it as a standard practice and improving team efficiency.
- Reviewed and approved merge requests for code developed by the team, ensuring high-quality deliverables and adherence to best practices in version control.

**Diligent Global Tech (February 2020 - November 2020)**
- Spearheaded the migration of legacy SSIS ETL pipelines to Talend Enterprise, modernizing data workflows and improving processing efficiency by 25%.
- Designed and developed end-to-end ETL pipelines in Talend, integrating flat file data sources (e.g., manufacturing beverage containers, retail sales) with Snowflake Data Warehouse, ensuring seamless data flow and accuracy.
- Independently managed the entire ETL lifecycle, from pipeline creation to deployment on Talend Administration Center (TAC), orchestration, and post-deployment support, ensuring 100% uptime during the warranty period.
- Optimized data ingestion and transformation processes, reducing pipeline execution time by 30% and enabling faster insights for business stakeholders.
- Provided end-to-end support for ETL pipelines, including troubleshooting, performance tuning, and resolving production issues, ensuring uninterrupted data delivery for critical business operations.
- Collaborated with cross-functional teams to understand data requirements, translating complex business needs into scalable and efficient ETL solutions.
- Established best practices for Talend ETL development, including reusable components, error handling, and logging, which improved maintainability and reduced future development efforts by 20%.
- Delivered two high-impact projects within tight deadlines, showcasing the ability to work independently and deliver results in a fast-paced startup environment.

**Optum Global Solutions (March 2011 - January 2020)**
- Pioneered the development of business-critical reports using SAP Crystal Reports, delivering actionable insights for Medicaid and Medicare data across 25 U.S. states, enabling data-driven decision-making for healthcare plans.
- Introduced and implemented Worksheet XML reports to address complex multi-spreadsheet reporting requirements, streamlining reporting processes and reducing manual effort by 40%.
- Spearheaded the adoption of Power BI for data analytics, conducting a successful proof-of-concept (POC) that identified trends in healthcare plans and enhanced strategic planning capabilities.
- Delivered high-stakes reports under tight deadlines, ensuring zero penalties (saving $100K per report) and maintaining 100% on-time delivery for mission-critical projects.
- Revolutionized data integration processes by introducing Talend ETL to the team, enabling seamless data aggregation from multiple sources and automating the generation of pre-filled template-based spreadsheets.
- Designed and implemented complex data pipelines to consolidate data from healthcare data marts (e.g., claims, members, providers), facilitating the creation of new healthcare products and improving business agility.
- Deployed onshore in the U.S. for two years, collaborating directly with clients to deliver clinical data reporting projects, ensuring alignment with business needs and fostering strong client relationships.
- Mentored and trained newcomers on SAP Crystal Reports, Power BI, and Talend ETL, fostering a culture of knowledge-sharing and skill development within the team.
- Optimized reporting workflows by automating data extraction, transformation, and loading (ETL) processes, reducing report generation time by 30% and improving data accuracy.
- Played a key role in strategic initiatives by providing data-driven insights that supported the launch of new healthcare products, contributing to the company’s growth and competitive edge.

---

## GitHub Stats

![GitHub Stats](https://github-readme-stats.vercel.app/api?username=sekhar546&show_icons=true&theme=dark&hide_border=true)

![Top Languages](https://github-readme-stats.vercel.app/api/top-langs/?username=sekhar546&layout=compact&theme=dark&hide_border=true)

---

## Daily Dev Stats

<a href="https://app.daily.dev/smokinguns47"><img src="https://api.daily.dev/devcards/v2/eE7hfVlRXjTv7mWhBD6GQ.png?type=wide&r=pjl" width="652" alt="Raja Sekhar R Gajjala's Dev Card"/></a>

---

## Learning Roadmap

[![roadmap.sh](https://roadmap.sh/card/wide/678d85c598c00f7117529a84?variant=dark)](https://roadmap.sh)

*Current focus: Scaling MLOps pipelines and real-time data systems*
